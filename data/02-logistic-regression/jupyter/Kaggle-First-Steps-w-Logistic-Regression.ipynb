{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import numpy and pandas, and DataFrame / Series\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "import scipy.optimize as optimize\n",
    "import scipy\n",
    "# Set some numpy options\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# Set some pandas options\n",
    "pd.set_option('display.notebook_repr_html', False)\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "# And some items for matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "#pd.options.display.mpl_style = 'default'\n",
    "\n",
    "# read http://scikit-image.org/docs/dev/user_guide.html \n",
    "# for documentation\n",
    "from skimage import io\n",
    "from skimage import color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = pd.read_csv(\"kaggle/trainLabels.csv\")\n",
    "m  = labels[\"ID\"].max()\n",
    "print (m)\n",
    "\n",
    "\n",
    "X = None\n",
    "img_size = 32\n",
    "for i in np.arange(1, m+1):\n",
    "    img = color.rgb2gray (io.imread(\"kaggle/train/{0}.Bmp\".format(i)))\n",
    "    img32 = scipy.misc.imresize(img, (img_size,img_size))\n",
    "    img32[img32<128] = 0\n",
    "    img32[img32>=128] = 1\n",
    "    img_array = np.array(img32).flatten()\n",
    "    img_array = img_array.reshape(1, img_array.shape[0])\n",
    "    if(X is None):\n",
    "        X = img_array\n",
    "    else:\n",
    "        X = np.vstack((X, img_array))\n",
    "    \n",
    "    #X = np.concatenate(([X,img32.flatten(1)]), axis=0)\n",
    "    \n",
    "print(X.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels_s = labels[\"Class\"].unique()\n",
    "np.ndarray.sort(labels_s)\n",
    "num_labels = labels_s.shape[0]\n",
    "print(labels_s.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "y = np.ones((m,1))*-1\n",
    "for i in np.arange(labels_s.shape[0]):\n",
    "    have_true = labels[\"Class\"]==labels_s[i]\n",
    "    for j in np.arange(m):\n",
    "        if(have_true[j] == True):\n",
    "            y[j] = i\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def displayData(X, example_width=None):\n",
    "#DISPLAYDATA Display 2D data in a nice grid\n",
    "#   [h, display_array] = DISPLAYDATA(X, example_width) displays 2D data\n",
    "#   stored in X in a nice grid. It returns the figure handle h and the \n",
    "#   displayed array if requested.\n",
    "\n",
    "    # Set example_width automatically if not passed in\n",
    "    if example_width == None:\n",
    "        example_width = round(math.sqrt(X.shape[1]));\n",
    "\n",
    "    # Compute rows, cols\n",
    "    m = X.shape[0]\n",
    "    n = X.shape[1]\n",
    "    example_height = round(n / example_width)\n",
    "    \n",
    "    # Compute number of items to display\n",
    "    display_rows = math.floor(math.sqrt(m));\n",
    "    display_cols = math.ceil(m / display_rows);\n",
    "\n",
    "    # Between images padding\n",
    "    pad = 1;\n",
    "    \n",
    "    # Setup blank display\n",
    "    display_array = - np.ones((pad + (display_rows * (example_height + pad)), pad + (display_cols * (example_width + pad))))\n",
    "\n",
    "    # Copy each example into a patch on the display array\n",
    "    curr_ex = 0;\n",
    "    for j in np.arange(display_rows):\n",
    "        for i in np.arange(display_cols):\n",
    "            if curr_ex > m : \n",
    "                break; \n",
    "            # Copy the patch\n",
    "\n",
    "            # Get the max value of the patch\n",
    "            max_val = np.amax(abs(X[curr_ex, :]));\n",
    "            display_array_startx = j * (example_width + pad) #pad + (j * (example_height + pad)) - 1\n",
    "            display_array_endx = ((j + 1) * (example_width + pad)) -1 # pad + (j * (example_height + pad))\n",
    "            display_array_starty = i * (example_height + pad) # pad + (i * (example_width + pad))\n",
    "            display_array_endy = ((i +1) * (example_height + pad)) - 1\n",
    "                    \n",
    "            de = (np.array(X[curr_ex, :]).reshape((example_width, example_height))) / max_val\n",
    "            #print de.shape\n",
    "            display_array[display_array_startx:display_array_endx, display_array_starty:display_array_endy] = (np.array(X[curr_ex, :]).reshape((example_width, example_height))) / max_val;\n",
    "            curr_ex = curr_ex + 1;\n",
    "        \n",
    "        if curr_ex > m:\n",
    "            break; \n",
    "\n",
    "\n",
    "    # Display Image\n",
    "    plt.imshow(display_array, extent = [0,100,0,100], aspect='auto', cmap=plt.get_cmap('gray'))\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#% Randomly select 100 data points to display\n",
    "rand_indices=np.random.permutation(m)\n",
    "sel = X[rand_indices[0:100], :]\n",
    "print(sel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "displayData(sel);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "#SIGMOID Compute sigmoid functoon\n",
    "#   J = SIGMOID(z) computes the sigmoid of z.\n",
    "    \n",
    "    g = 1.0 / (1.0 + np.exp(-z));\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lrCostFunction(theta_unraveled, X, y, lamda):\n",
    "#LRCOSTFUNCTION Compute cost for logistic regression with \n",
    "#regularization\n",
    "#   J = LRCOSTFUNCTION(theta, X, y, lambda) computes the cost of using\n",
    "#   theta as the parameter for regularized logistic regression and the\n",
    "#   gradient of the cost w.r.t. to the parameters. \n",
    "\n",
    "# Initialize some useful values\n",
    "    m = y.shape[0] # number of training examples\n",
    "\n",
    "# You need to return the following variables correctly \n",
    "    J = 0\n",
    "\n",
    "# ====================== YOUR CODE HERE ======================\n",
    "# Instructions: Compute the cost of a particular choice of theta.\n",
    "#               You should set J to the cost.\n",
    "#               Compute the partial derivatives and set grad to the partial\n",
    "#               derivatives of the cost w.r.t. each parameter in theta\n",
    "#\n",
    "# Hint: The computation of the cost function and gradients can be\n",
    "#       efficiently vectorized. For example, consider the computation\n",
    "#\n",
    "#           sigmoid(X * theta)\n",
    "#\n",
    "#       Each row of the resulting matrix will contain the value of the\n",
    "#       prediction for that example. You can make use of this to vectorize\n",
    "#       the cost function and gradient computations. \n",
    "#\n",
    "# Hint: When computing the gradient of the regularized cost function, \n",
    "#       there're many possible vectorized solutions, but one solution\n",
    "#       looks like:\n",
    "#           grad = (unregularized gradient for logistic regression)\n",
    "#           temp = theta; \n",
    "#           temp(1) = 0;   % because we don't add anything for j = 0  \n",
    "#           grad = grad + YOUR_CODE_HERE (using the temp variable)\n",
    "#\n",
    "    theta_dim0 = theta_unraveled.shape[0]\n",
    "    theta_dim1 = 1\n",
    "    if theta_dim0 == 1:\n",
    "        theta_dim1 = theta_unraveled.shape[1]\n",
    "    \n",
    "    #print(\"hola3:\", theta_unraveled.shape)\n",
    "    theta = None\n",
    "    if theta_dim0 > theta_dim1:\n",
    "        theta = np.reshape(theta_unraveled, (theta_dim0, 1))\n",
    "    else:\n",
    "        theta = np.reshape(theta_unraveled, (theta_dim1, 1))\n",
    "\n",
    "    #print(\"hola:\", theta.shape)\n",
    "\n",
    "    tmp = 1/m*np.sum(-(y*(np.log(sigmoid(X.dot(theta)))))-(1-y)*(np.log(1-sigmoid(X.dot(theta)))))\n",
    "    J = tmp + (lamda/(2*m))*(theta[1:].T.dot(theta[1:]) ) \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lrCostGradFunction(theta_unraveled, X, y, lamda):\n",
    "#LRCOSTFUNCTION Compute gradient for logistic regression with \n",
    "#regularization\n",
    "# Initialize some useful values\n",
    "    m = y.shape[0] # number of training examples\n",
    "    \n",
    "    theta_dim0 = theta_unraveled.shape[0]\n",
    "    theta_dim1 = 1\n",
    "    if theta_dim0 == 1:\n",
    "        theta_dim1 = theta_unraveled.shape[1]\n",
    "    \n",
    "    #print(\"hola4:\", theta_unraveled.shape)\n",
    "    theta = None\n",
    "    if theta_dim0 > theta_dim1:\n",
    "        theta = np.reshape(theta_unraveled, (theta_dim0, 1))\n",
    "    else:\n",
    "        theta = np.reshape(theta_unraveled, (theta_dim1, 1))\n",
    "\n",
    "    #print(\"hola5:\", theta.shape)\n",
    "    \n",
    "    gradient_a = 1/m*((sigmoid(X.dot(theta))-y).T.dot(X))\n",
    "    gradient_b = (lamda/m)*(theta)\n",
    "    gradient_b[0]=0;\n",
    "    grad = gradient_a + gradient_b.T;\n",
    "# =============================================================\n",
    "    grad = grad.flatten(1)\n",
    "    #print(\"hola2: \", grad.shape)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def oneVsAll(X, y, num_labels, lamda):\n",
    "#ONEVSALL trains multiple logistic regression classifiers and returns all\n",
    "#the classifiers in a matrix all_theta, where the i-th row of all_theta \n",
    "#corresponds to the classifier for label i\n",
    "#   [all_theta] = ONEVSALL(X, y, num_labels, lambda) trains num_labels\n",
    "#   logisitc regression classifiers and returns each of these classifiers\n",
    "#   in a matrix all_theta, where the i-th row of all_theta corresponds \n",
    "#   to the classifier for label i\n",
    "\n",
    "# Some useful variables\n",
    "    m = X.shape[0]\n",
    "    n = X.shape[1]\n",
    "\n",
    "# You need to return the following variables correctly \n",
    "    all_theta = np.zeros((num_labels, n+1))\n",
    "\n",
    "# Add ones to the X data matrix\n",
    "    X = np.hstack((np.ones((m,1)), X))\n",
    "\n",
    "# ====================== YOUR CODE HERE ======================\n",
    "# Instructions: You should complete the following code to train num_labels\n",
    "#               logistic regression classifiers with regularization\n",
    "#               parameter lambda. \n",
    "#\n",
    "# Hint: theta(:) will return a column vector.\n",
    "#\n",
    "# Hint: You can use y == c to obtain a vector of 1's and 0's that tell use \n",
    "#       whether the ground truth is true/false for this class.\n",
    "#\n",
    "# Note: For this assignment, we recommend using fmincg to optimize the cost\n",
    "#       function. It is okay to use a for-loop (for c = 1:num_labels) to\n",
    "#       loop over the different classes.\n",
    "#\n",
    "#       fmincg works similarly to fminunc, but is more efficient when we\n",
    "#       are dealing with large number of parameters.\n",
    "#\n",
    "# Example Code for fmincg:\n",
    "#\n",
    "#     % Set Initial theta\n",
    "#     initial_theta = zeros(n + 1, 1);\n",
    "#     \n",
    "#     % Set options for fminunc\n",
    "#     options = optimset('GradObj', 'on', 'MaxIter', 50);\n",
    "# \n",
    "#     % Run fmincg to obtain the optimal theta\n",
    "#     % This function will return theta and the cost \n",
    "#     [theta] = ...\n",
    "#         fmincg (@(t)(lrCostFunction(t, X, (y == c), lambda)), ...\n",
    "#                 initial_theta, options);\n",
    "#\n",
    "#    for i in np.arange(num_labels):\n",
    "    for i in np.arange(num_labels):\n",
    "        print(\"{0} of {1}\".format(i+1, num_labels))\n",
    "        initial_theta = np.zeros((X.shape[1], 1))    \n",
    "        Result = optimize.minimize(fun = lrCostFunction, x0 = initial_theta, args = (X, y==i, lamda), method = 'CG',\n",
    "                                 jac = lrCostGradFunction)\n",
    "        optimal_theta = Result.x\n",
    "        all_theta[i,:] = optimal_theta.flatten(1)\n",
    "    \n",
    "    return all_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predictOneVsAll(all_theta, X):\n",
    "#PREDICT Predict the label for a trained one-vs-all classifier. The labels \n",
    "#are in the range 1..K, where K = size(all_theta, 1). \n",
    "#  p = PREDICTONEVSALL(all_theta, X) will return a vector of predictions\n",
    "#  for each example in the matrix X. Note that X contains the examples in\n",
    "#  rows. all_theta is a matrix where the i-th row is a trained logistic\n",
    "#  regression theta vector for the i-th class. You should set p to a vector\n",
    "#  of values from 1..K (e.g., p = [1; 3; 1; 2] predicts classes 1, 3, 1, 2\n",
    "#  for 4 examples) \n",
    "\n",
    "    m = X.shape[0]\n",
    "    num_labels = all_theta.shape[0]\n",
    "\n",
    "# You need to return the following variables correctly \n",
    "    p = np.zeros((X.shape[0], 1))\n",
    "\n",
    "# Add ones to the X data matrix\n",
    "    X = np.hstack((np.ones((m,1)), X))\n",
    "\n",
    "# ====================== YOUR CODE HERE ======================\n",
    "# Instructions: Complete the following code to make predictions using\n",
    "#               your learned logistic regression parameters (one-vs-all).\n",
    "#               You should set p to a vector of predictions (from 1 to\n",
    "#               num_labels).\n",
    "#\n",
    "# Hint: This code can be done all vectorized using the max function.\n",
    "#       In particular, the max function can also return the index of the \n",
    "#       max element, for more information see 'help max'. If your examples \n",
    "#       are in rows, then, you can use max(A, [], 2) to obtain the max \n",
    "#       for each row.\n",
    "#       \n",
    "\n",
    "    p =np.argmax(X.dot(all_theta.T), axis=1)\n",
    "    p = p.reshape((m,1))\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print ('\\nTraining One-vs-All Logistic Regression...\\n')\n",
    "\n",
    "lamda_vec = [0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10]\n",
    "acc = 0\n",
    "winning_theta = None\n",
    "winning_lamda = None\n",
    "for la in lamda_vec:\n",
    "    print('Trying Lambda {0}\\n'.format(la))\n",
    "    all_theta = oneVsAll(X, y, num_labels, la)\n",
    "    pred = predictOneVsAll(all_theta, X)\n",
    "    acc_tmp = np.mean(pred == y) * 100\n",
    "    print('\\n>>>>>>\\nFor Lambda {0} Training Set Accuracy: \\n{1}\\n<<<<<\\n'.format(la,acc_tmp))\n",
    "    if acc_tmp > acc:\n",
    "        acc = acc_tmp\n",
    "        winning_theta = all_theta\n",
    "        winning_lamda = la\n",
    "\n",
    "print('\\nBest Training Set Accuracy with Lambda {0}: \\n{1}'.format(winning_lamda, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# With img_size = 32\n",
    "# With color.rgb2gray\n",
    "# Best Training Set Accuracy with Lambda 1: \n",
    "# 13.7673086105\n",
    "#---------------------\n",
    "#---------------------\n",
    "# With colors either 1 or 0\n",
    "# Best Training Set Accuracy with Lambda 0.001: \n",
    "# 18.749005252268024"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
